# 머신러닝 상품화

# 상품화는 복잡하다

상품화를 위한 모델은 모델학습만 잘하면 될까요?

1. 모델 개발을 위한 과정

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled.png)

데이터 준비 및 결정 및 EDA 분석

- 모델 준비
- 모델 학습
- 모델 평가등 과정

2. 크게보면 3단계로 구분

학습 데이터 준비

1. Feature Engineering
    1. 모델에 학습할수 있는 형태로 engineering합니다.
    2. Data Cleansing, Feature Selection , Feature Reduction , Data Augmentation , Data Scaling & Encoding
2. 모델 학습 및 최적화 평가

3. 모델 개발 사례

숙박업소 추천모델이라면?

- 숙박 추천 문제 이해
    - 사용자 프로필, 검색기록등을 활용하여 맞춤형 숙박추천 서비스 제공
- 모델 개발 목표 설정
    - 사용자 경험을 증가시키기 위한 숙박추천 서비스 개발하고자 함
    - 이를 위해 정확성과, 속도 모두 높은 머신러닝 모델 개발

**학습 데이터 준비**

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%201.png)

- 데이터셋 수집 방법 설계
    - 호텔 정보나, 리뷰데이터등
- 개인정보 보호 검토
    - 개인정보 보호 검토 필요
- 숙박 데이터 수집
    - 크롤링시 저장된 스토리지와 스토리지 유형 고려
- 레이블링
    - 정답 data를 구성하지만 정답이 없는 경우 어떻게 이 문제를 해결할지 검토

Feature Engineering

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%202.png)

데이터 정제 및 결측치 처리

- 데이터 분석
- 변환 및 스케일링
- 데이터 분할 및 검증세트 구성

모델 훈련과 선택

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%203.png)

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%204.png)

알고리즘 선택

1. 모델 선택 process
    1. 평가지표를 설정: 일부만 추천하던가 ,리뷰가 적은 호텔도 추천하던가 다양한 방식으로 선택할수있는 모델 알고리즘 성능 비교
2. 모델 훈련 및 성능 평가

모델 성능 향상 및 최적화

하이퍼 파라미터 튜닝

- 앙상블 기법
- 오차 분석 및 해결

모델 상품화 검토

성능평가 결과

- ex) 정확도는 몇퍼이상이며 해석가능성이 있어야한다.
- 성능 개선 가능성
    - 데이터 양 증가, 특징 추가 ,알고리즘 도입등
- 적용 가능성 검토
    - 추론속도, 지연시간, AB 테스트 등

# 학습 데이터 준비

데이터 유형

정형 데이터

- 정해져있는 데이터
- 표형태 , 고정된 스키마
- 데이터베이스, csv같은 파일
- 특징:
    - SQL 쿼리 가능
    - 정해진 필드 및 데이터 형식 갖음
    - 쉬운 분석
- 비정형 데이터
    - 구조가 없거나 매우 제한적인 구조를 가지지 않는 데이터
    - 텍스트 ,이미지, 오디오, 비디오 같은 형태를 가짐
    - 특징:
        - 데이터는 자유형식, 구조따르지 않음
        - 고도의 전처리 필요  : 데이터 전처리 및 특징추출
- 반정형 데이터
    - 구조가 명확하게 정의되지 않는 데이터로 일부 구조화된 정보를 가짐
    - XML, JSON, HTML같은 마크업 언어를 사용하여 주로 표현
    - ex) 웹 스크랩 데이터, 기업문서 데이터
    - 특징:
        - 일부 구조화,
        - 데이터 파싱 필요
        - 유연성 : 데이터 형식이 유연하며 새로운 정보추가하거나 수정 용이

2. 실시간 데이터

DBMS

- 구조화된 데이터를 효율적으로 저장, 관리 및 검색하기 위한 software 시스템
- SQL을 사용하여 데이터베이스에 대한 쿼리 및 조작을 지원
- MYSQL, PostgresSQL

데이터를 실시간으로 분석하고 수행해야 하는 데이터

- 이미 저장된 것이 아닌 실시간으로 데이터가 수집되늰 것
- 유튜브 시청 데이터 , IOT 센서 데이터 ,금융거래 데이터 등

예시 : 센서로 환경데이터 수집시 이상이 생길시 바로 알림등

- 스트리밍 데이터 개념

지속적으로 생성되고 전송되는 데이터 스트림

- 시간이 지나면서 데이터가 계속해서 이전 데이터에 추가되는것
    - 데이터 용량 증가

처리의 관련 특징

- 무한 스트리밍 데이터 : 지속적으로 데이터를 처리하기 위한 특수방법 필요
- 실시간 분석 : 데이터가 도착하자마자 분석하며 지연발생 X

3. 데이터 저장과 관리

데이터 저장소 종류

DBMS(data base)

- Nosql Database
- Data Warehouse
- Data Lake

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%205.png)

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%206.png)

- NoSQL DB
    - 정형 + 반정형 및 비정형 데이터 저장 관리 용이
    - 데이터 형식 유연성 제공, 대량 데이터 처리와 분산 환경 확장 가능
    - MongoDB, Cassndra, Redis
- Data Lake
    - **다양한 비구조데이터를 저장하는 시스템으로 원시 데이터 보관하고 나중에 분석 및 처리에 활용**
    - 스키마에 유연성을 제공하고 나중에 정의되거나 변경될수 있으며 쉽게 새로운 데이터 형식을 수용가능
- Data Warehouse
    - 다양한 데이터 원본에서 데이터 추출, 변경 및 로드 (ETL)하여 중앙집계 및 분석을 위한 중앙 데이터 저장소를 만드는 시스템
    - 주로 정형 데이터 중심, 구조화된 데이터를 저장하고 분석

![SQL vs NoSQL](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%207.png)

SQL vs NoSQL

데이터 버젼 관리

형상관리

- 데이터 변경이력을 추적하고 이전 상태로 롤백할수 있게 하는 process
- 데이터의 안정성과 무결정을 보장하며 잘못된 변경으로 인한 문제를 예방
- Feature Store
    - Feature의 버젼 관리를 지원

데이터 품질과 보안

데이터 품질이 좋아야 의사결정이 좋아짐

- 잘못된 분석 및 결정을 초래할수 있음
- 따라서 데이터 품질 및 검사 개선 방법이 필요함
- 데이터 클리닝, 중복제거 ,이상치 감지 및 데이터 표준화 같은것
- 데이터 보안을 위한 방어적 조치
    - 암호화 ,액세스 제어, 물리적 보안, 네트워크 보안, 모바일 기기 보안, 정책 및 교육등이 포함

데이터 보안?

데이터를 무단 액세스 ,변조, 유출 ,파괴 보호

- 데이터 유출은 법적문제로 고려 필요

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%208.png)

# Data Sampling

# Data Sampling이란?

- 큰 데이터 집합에서 작은 데이터 부분 집합을 추출하는 프로세스
- 계산공간을 줄이는 데 도움
- 통계및 데이터 분석에서 활용되는 일반적인 기술로 데이터 일부를 조사하고 결론을 전체 데이터에 대한 집함의 결론으로써 활용

## 목적

- 자원 및 시간 절약
- 품질 향상
- 통계적 추론
- 데이터 시각화
- 데이터 테스트

# 2. Data Sampling 종류

### Radom Sampling

- 무작위로 데이터 집합에서 샘플 선택
- 편향이 적게 대표있는 샘플을 얻을수 있음

### Startified Sampling

- 데이터를 계층적으로 분류한 후 , 각 계층에서 샘플을 추출하는 방법
- 각 계층의 특성을 고려하여 샘플을 얻기 위해 사용
- 예를 들어 남성과 여성의 성별에 따라 샘플을 추출

### Cluster Sampling

- 데이터를 여러 그룹으로 나누고 몇개의 cluster를 무작위로 선택한 후 선택된 cluster 내의 모든 데이터를 포함하는 방법
- 데이터가 고루 분포되지 않은 경우에 유용
- 데이터가 클러스터로 그룹화 될때 사용

## Weight Sampling

- 데이터 포인트를 가중치 할당하고 가중치를 기반으로 샘플을 추출
- 데이터 포인트는 해당 데이터의 중요도이며 중요한 데이터는 더 자주 선택될 가능성이 높음
- 불균형한 데이터 분포를 가질 경우 잘 활용됌

## Importance Sampling

- 확률 분포에 기반한 통계 샘플링 기법
- 베이지안 추론, 몬테카를로 시뮬레이션, 결합 확률 분포등

# 3. 샘플링시 고려사항

- 편향과 오차관리
    - 특정 데이터 세트에 과소, 과대 표현이 일어나지 않도록
    - 샘플링 과정속에서 무작위성의한 변동이 많지 않도록
- 데이터 샘플링 샘플 크기 및 신뢰 수준
    - 샘플 크기와 신뢰 수준은 중요한 결정사항
    - 통계적 방법 사용
    - 예를 들어 모집단의 크기, 신뢰수준등 샘플 크기 선정

# Labeling과 이에 따른 모델 학습 유형

## 라벨링

- 라벨 이름 붙이기 , 특정 객체에 의미부여 프로세스임

## 데이터 라벨링

- 기계학습 및 딥러닝 모델 훈련하기 위한 데이터 의미 부여
- 컴퓨터가 데이터를 처리하고 해석할수 있는 방식으로 데이터 변환 작업

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%209.png)

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2010.png)

# AI모델 학습 유형

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2011.png)

# Supervised Learning

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2012.png)

# Semi-Supervised

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2013.png)

# Self Training?

- 초기에 라벨링 데이터로 모델훈련후 ,모델이 높게 예측한 라벨이 없는 데이터에 적용하여 라벨링하고 학습하는 과정 반복

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2014.png)

# Co Traning?

- 데이터를 여러 독립적인 부분 , 도메인으로 나누어 모델학습하고 semi-supervised학습에서의 데이터 부족 문제 해결
1. 초기 데이터 : 데이터를 여러 부분으로 나눔 (텍스트 : 다른 문장, 문서를 별도의 부분으로 분해)
2. 초기 모델 : 독립된 모델의 각 부분에 대해 각각 학습 , A는 하나의 도메인으로 학습 ,B는 다른 도메인으로 학습
3. 정보 교환 : 모델 A와 B가 각자 학습한 결과물을 공유하며 서로 예측 보완

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2015.png)

# Multiview Learning

- co와 비슷하지만 데이터 안의 속성을 나눔
- 데이터를 여러 관점 또는 특성을 나누어 모델 학습
- 데이터의 다양한 특성이 중요한 경우에 유용
1. 데이터 분할
2. 모델 학습
3. 결합

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2016.png)

# Self-Supervised Learning

- 라벨이 없는 데이터로 모델을 훈련
- 데이터 내에서 숨겨진 정보를 활용하여 모델 학습
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2017.png)
    

## AutoEncoder

- 입력데이터 압축후 복원하는 아키텍쳐

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2018.png)

## Masked Language Model

- 일부 단어를 mask하고 예측하는 task활용
- 문맥을 이해하고 숨겨진 단어를 예측하기 위한 단어간의 관계를 학습
- 자연어 처리모델의 사전훈련에 많이 쓰임

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2019.png)

## Contrastive Learning

- 모델에게 유사한 데이터는 가깝게, 다른 데이터는 멀게 표현하도록 학습시키는 방법
- 모델은 **데이터간의 유사성을 학습**하여 서로 다른예제를 구분하는 방법

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2020.png)

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2021.png)

# 모델 학습 유형

- Transfer Learning
- Finetuning Learning
- Online learning & Batch Learning

## Online learning

- 데이터를 순차적으로 처리하면서 모델 업데이트
- 새로운 데이터 도착할때마다 모델 finetuning 하며 스트리밍데이터나 지속적인 학습을 위해 사용

# 모델학습 유형 선택 고려사항

- 데이터 양 ,학습시간, 모델 복잡성, 도메인 특성등 고려하여 전략적 선택
- 이와같은 고려는 MLOps에서 활용되어야함

# Class Imbalance 해결법

- 데이터에서 발생하는 현상중 하나로 데이터 클래스간의 불균형
- 데이터 포인트수가 현저히 적을때
    - 희귀질병 : 질병이 있는 경우가 드물게 바랭
- 발생하는 이유는 ?
    - 현실세계의 데이터 불균형 발생 → 대부분의 거래가 정상적인 신용카드 거래, 사기거래 드물게 발생
    - 데이터 수집과정에서 클래스 불균형 발생 , 특정 클래스에 대한 sampling 오류
- Data 수집 방법 , labeling 오류

## Class Imbalance의 중요성

- 실제 ML 응용분야에서 많이 발생
1. 모델 편향
    1. 모델이 소수 클래스를 올바르게 식별하지 못함
2. 비용 고려
    1. 소수 클래스의 중요도가 높은 경우 이를 반영하지 않으면 예측 비용이나 성능이 실제 상황가 맞지 않을수 있음
3. 평가지표의 왜곡
    1. 정확도같은 평가지표가 왜곡될수 있으며 실제 모델의 성능을 평가하기 어렵게 만듬

## 모델 성능지표에 미치는 영향

- 평가 지표의 왜곡
    - 모든 예측이 부정이라는 방식으로 정확도가 높게 나올수 있음
- Precision ,Recall  ,f1score등의 성능지표를 고려애햐함
- ROC, AUC curve등 고려해야함
    - ROC : FPR에 대한 TPR의 변화
    - AUC : ROC의 아래 넓이

## 목표?

- 소수 클래스에 대한 모델 성능 향상
- 오분류 비용 줄이고 모델의 실용성을 향상 시킬수 있음

### 방법론

- 데이터 기반 접근
    - Resampling을 통한 데이터 수 조절
- ML 알고리즘 기반 접근
    - model 파라미터를 조절하여 클래스 imbalance를 고려하도록 모델 tuning

# Resampling

- 데이터 수 조절하여 클래스 불균형 다룸
- Oversampling, Under Sampling을 통한 클래스 조정

## Resampling 종류

- oversampline, undersampline , Combined sampline
- Over : 소수 클래스 데이터 증가
- Under : 다수 클래스 데이터를 감소시키는 방법

## Resampling시 주의사항

- 기법의 장단점을들 고려해야함
- Over : 데이터를 확장하지만 데이터 중복및 과적합 문제 발생가능
- Under : 다수 클래스를 줄이지만 정보손실 발생 가능

# 모델 튜닝의 필요성

- 클래스 불균형을 고려한 모델 tuning
- 모델 소수 클래스를 더잘 식별할수 있게 해줌

### Weight 조정

- 가중치를 조절하여 , 소수 클래스에 대한 높은 중요도 부여 가능
- 모델은 소수 클래스를 더 잘 식별할수 있게 됌

## Threshold 조정

- 분류 모델에서 임계값을 조정하여 모델 평가 지표를 향상시킬수 있음
- 예를 들어 recall을 높이기 위해 임계값 조정

# OverSampling

## SMOTE(Synthetic Minority Over-sampling)

- 소수 클래스 데이터 포인트들을 기존 뎅이터를 활용하여 균형화함
- 새로운 데이터 포인터는 소수 클래스 데이터 포인트와 그 주변의 이웃 데이터 포인터 사이를 보간하여 생성
- 장점: 다양한 데이터 생성 하여 모델 일반화 성능 증가
- 단점 : 데이터 중복 발생

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2022.png)

## ADASYN (Adaptive Synthetic Sampling)

- 소수 클래스 데이터 포인트들을 기존 데이터를 활용함
- 소수 클래스 데이터포인터ㅓ 가중치를 계산하고 높은 가중치를 가지는 데이터 포인터에 대해 더 많은 합성 수행
- 가중치는 데이터 분포와 클래스간 거리에 따라 동적으로 조정, 가까운 데이터포인터에 대해 합성이 더욱 강조됌
- 장점 : 데이터 거리에 더 적응적이며 , 클래스간 거리에 따라 합성을 조절하여 불균형을 더 효과적으로 다를수 있음
- 단점 : SMOTE에 비해 복잡하여 계산 비용 높을수 있음

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2023.png)

# UnderSampline

## Randoom UnderSampling (많이 쓰이진 않음)

- 무작위로 데이터 선택하여 소수클래스와의 균형 맞춤
- 장점 : 간단함
- 단점 : 정보 손실발생하고 ,선택된 데이터가 소수클래스 대표 하지 못할수 있음

## Tomek Links UnderSampling

- 클래스간 거리가 가까운 데이터 포인트 쌍중에서 다수 클래스를 제거하는 방법
- 수행과정
    1. 다수 클래스와 소수클래스 사이의 모든 데이터포인트 거리 계산
    2. Tomek Link 식별: 서로 다른 클래스에 속한 데이터 포인트 쌍중 클래스간 거리가 가장가까운 데이터포인트 쌍의미
    3. Tomek Links 제거 : 식별된 Tomek Links 데이터에서 제거
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2024.png)
    

## ENN (Edited Nearest Neighbors)

- 클래스간 거리가 가까운 데이터 포인트 쌍 중에서 다수 클래스 데이터를 제거하는 방법
- 수행과정
    1. 다수 클래스 데이터포인터에 대해 해당 데이터 포인트와 가장 가까운 k개의 이웃을 잡음
    2. 다수 클래스 데이터 포인터 중에서 소수 클래스로 잘못 분류된 데이터포인트 식별
    3. 이런 오분류된 데이터 포인터를 제거하여 다수 클래스 데이터 정제
    4. 모델이 잘못 분류하는 오분류 데이터를 제거하고 모델의 성능 향상
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2025.png)
    

## TomekLinks vs ENN

- 공통점
    - 클래스간거리 고려 하여 데이터 포인트를 수정함
- 차이점
    - TomekLinks 는 “서로다른 클래스간의 거리”를 이용행서 데이터 쌍을 식별하여 제거
    - ENN은 다수클래스에 대해 가장 가까운 이웃을 기반으로 오분류 된 데이터를 찾아내서 제거
    - Tomek Links는 클래스간 “경계” 명확히
    - ENN은 다수 클래스 데이터를 정제하고 오분류를 줄이는 것이 목적

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2026.png)

# Combined Sampling

## SMOTEENN

- SMOTE와 ENN 합성
- 동작 방식
1. SMOTE를 활용한 oversampling
2. ENN을 활용한 under sampling
3. 균형된 데이터셋 구현
- 장점 : 클래스 불균형 효과적으로 해결가능
- 당점: 모델 성능은 데이터셋의 특성과 문제에 따라 정보손실이 발생할수 있으며, 실험과 비교분석이 필요

# 5. ML 알고리즘 기반 방법론

- 모델 튜닝 및 파라미터 조정
- 모델 알고리즘 선택, 클래스 가중치 조정, 임계값 조정 , 적절한 Metric 사용 ,Hyper parameter Tuning

# ML알고리즘 선택

- class imbalance에 Sensitive한 모델 알고리즘 선택
- Binary Classification에서는 SVM. RandomForest, Gradient Boost같은 모델 성능이 좋음

## Class Weight 조정

- 모델 Loss function에 class weight 를 부여하여 imbalance class에 높은 가중치 할당
- 모델 imbalance dataset를 더 중요하게 취급하여 학습 진행

## Threshold 조정

- 모델의 예측 threshold를 조정하여 precision과 recall 조정

## 적절한 Metric 사용

- Accuracy가 일반적으로 imbalance class에서 부적합함으로 , 다른 Metric기반으로 모델 최적화

## Hyper-parameter 튜닝

- 모델 튜닝을 통해서 최적의 모델 찾음

## Validation

- Cross Validation을 통해 일반화 성능을 높이고 Class imbalance에 대해서도 신뢰 가능성을 확인 필요

## Ensemble 활용

- 모델 결합을 통해 Class imbalance활용

# 6. Class Imbalance 성능평가 및 선택

## 성능 평가의 중요성

- 모델의 성능을 평가하는 것은 매우 중요한 부분

## 평가 Metric 선택

- 모델의 목적과 활용에 따라서 선택해야함

## Precision

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2027.png)

## Recall

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2028.png)

## F1-score

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2029.png)

## ROC Curve

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2030.png)

## AUC

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2031.png)

# MLOps에서 Class Imbalance통합

## 통합의 이유

- 모델의 정확성 및 신뢰성에 영향을 줌
- 방법
    - 데이터 수집 및 전처리 단계
        - 클래스 불균형을 고려하여 데이터 수집 및 전처리 과정에서 적절한 Resampling필요
    - 모델 훈련 단계
        - 모델 훈련과정에서 가중치 조정 및 성능지표 설정을 통해 고려
    - 평가 및 모니터링
        - 모델의 실시간 평가 및 모니터링에서 모델이 불균형한 input에도 정상적으로 동작하는 지 확인 필요

## Model update에서 Class imbalance 관리

- 모델의 시간적 변화
    - 실제 환경에서 데이터 클래스 분포가 시간이 지남에 따라 변함
- 모델 update와 Retraining 전략
    - 정기적 모델 update및 스트리밍 데이터를 모델에 retrainign 하는 방법 고려해야함
- Data drift 및 class imbalance :
    - 데이터 분포의 변경 (Drift)를 고려해야함
- 모델버젼 관리
- 모델 성능 모니터링
- 자동화 Update 및 Retraining

# Feature Engineering # Data Cleansing

# Data Cleansing 이란?

- 데이터 전처리중하나
- 원시 데이터를 모델 학습에 적합한 형태로 가공하는 과정
    - 노이즈 데이터, 이상치 데이터, 누락된 데이터 처리해서 모델이 데이터 분포를 잘 학습할수 있도록 하는 전처리 과정

## Data Cleansing 목적

- 데이터의 정확성과 품질을 확보하는 것
- 모델 학습에 영향을 끼치는 Nosiy data , Anomaly Data , N/A data를 식별하고 처리하여 모델 성능 향상

## Noise Data 식별

- 노이즈 데이터로 성능 저하시키는 데이터 식별하는 목적
- 노이즈 데이터는 오류나 무작위한 값으로 인해 데이터셋을 혼란스럽게 만들수 있음
- 처리:
    - 데이터 시각화 하고 통계적 방법사용하고 이상치탐지 방식을 사용하여 식별하여 처리

## Anomaly Data 식별

- 다른 데이터와 너무나 다른 데이터 , 모델 학습에 부정적 영향 끼침
- 처리 :
    - 이상치 제거 및 대체
    - 이상치 제거가 전반적으로 사용됌 , 대체는 데이터 분포를 왜곡할수 있음

## 누락 Data 식별

- 다른 데이터에는 있지만 일부 데이터에 누락된 속성을 식별하여 보정
- 처리
    - 제거된 데이터 완전성을 유지하나 데이터수가 줄어들거나 class imbalance가 심해질수 있음
    - 대체는 평균, 중간값, 최빈값 등의 대체 방법, 그러나 잘못된 대체로 데이터 분포 왜곡할수 있음

# 3. MLOps에서 Data Cleansing

## MLOps 자동화

- 데이터 클렌징 자동화 하고 관리함으로써 모델의 신뢰성 증가
    - 데이터 전처리 프로세스를 효율적으로 통합할수 있음
- 데이터 전처리 파이프라인 구축하고 모니터링하여 데이터 변화에 맞게 빠르게 대응하고 모델 배포 및 관리에도 긍정적 영향을 제공

# Feature Engineering # Feature Selection

# 1. Feature Selection의 중요성 # 통계 기반

- 가장 관련성이높거나 유용한 특성 선ㄴ택 및 불필요한 특성 제거
- 머신러닝 모델을 개선하고 모델 효율성을 향상시킬수 있음
- 차원의 저주:
    - 차원 증가할수록 데이터간의 의미가 사라짐
- 계산 효율성
    - 중요하지 않은 특성을 제거하면서 계산 효율성 향상
- 모델의 해석:
    - 중요한 특성만 사용하면 모델의 해석이 쉬워짐

# 2. 통계 기법 소개

## 통계기법 종류

- ANOVA
    - 일원 분산 분석
- Chi-Squared Test
    - 카이 제곱 검정

 

## ANOVA (일원 분산 분석)

- 카테고리별 numeric data의 분포 차이를 검증하는 방법
- 성별에 따른 키 분포 분석검증
- 전제  : **정규성, 등분산성, 독립성** → 만족하지 않으면 해당 검정 신뢰 불가능
- 원리 :
    - 두개이상의 그룹간 평균 차이가 통계적으로 의미있는지 평가하는 방법
    - 계산식
        - F = Between Group variance / With Group variance
    - 해석
        - F값이 크면 평균 차이가 통계적으로 유의미하다고 볼수 있음
        - p-value값을 통해 통계적 의미성을 확인할수 있음 (0.05이하일경우)

## 카이 제곱 검정

- 두 범주형 변수에 대한 분석 방법
- 예를 들어 성별에 따른 선호영화 장르 비교 문제
- 카이제곱 검정 3가지 방법
    - 적합도검정
        - 변수 1개인 경우
        - 기존에 알려준 기준이 존재하는 검정
            - 상자안에 공 3개가 같은 비율로 있다 . 공 100개를 두고 했을때 각 색의 비율구함 → 기존의 알려진 공 비율분포를 따르는지에 대한 검정
    - 독립성 검정
        - 변수 2개인 경우
        - 범주형 두변수가 서로 연관되어있는지 여부를 결정
            - 성별과 흡연 여부 관계를 알고 싶어서 200명을 추출하여 조사한 경우
    - 동질성 검정
        - 변수 2개인 경우
        - 범주형 변수 사이의 관계를 알기위한 검정은 아니고 각 그룹이 동질한지 알고 싶은 검정
            - 남자와 여자 흡연율의 차이가 있는지 흡연율을 조사한 후 , 두 그룹의 흡연율이 같은지 여부를 검정
        

# 2. Feature Selection # 데이터 기반

# Pearson Correlation Coefficient

- 피어슨 상관계쑤?
- 두 변수 간의 선형관계의 강도와 방향을 나타내는 값
- -1~1 :
    - 음의 상관관계~양의 상관관계
- 주의점
    - 상관계수가 높다고 해서 두변수간의 인과관계가 있다고 결론짓기는 어려움

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2032.png)

# Variance Inflation Factor?

## VIF Analysis란?

- 회귀분석에서 독립변수들 간의 다중 공선성을 평가하는데 사용되는 값
    - 즉 두개이상의 독립변수가 밀접하게 관련되어 있는 상황을 평가하는데 사용되는값
- VIF는 해당 변수가 다른 변수가 얼마나 관련되어있는지 수치로 표현
    - VIF = 1 : 변수들간의 전혀 다중공선성이 없음
    - VIF > 10: 보통 다중공선성이 있다고 판단하고 주의가 필요함
- 예시 : 집의 면적과 방의수 , 둘의 가능성이 높음
- 해결방법
    - VIF 값이 높은 변수들중 하나나 더 많은 변수를 제거
    - 변수를 결합하여 새로운 변수를 만듦
    

# 3. Feature Selection # ML기반

# ML Feature Selection

- 여러 알고리즘이 자체적으로 feature importance를 평가하고 선택할수 있는 능력을 가짐
- 데이터 내제된 복잡한 관계를 바탕으로 중요한 특성들만 선택할수 있음

## 1. Decision Tree Based

- 어떤 변수가 node분할에 중요한 역할을 하는지 기반을 Feature Importance 평가
- 구체적인 방법
    - Decision Tree Learnring
    - 각 node에서 feature분할로 인한 불순도 감소를 합산

## 2. Random Forest based

- 이미 여러개의 DecisionTree들의 feataure importnace를 평균내어 feature importance를 평가하는 방법
- 구체적인 방법
    - 훈련
    - 각 Tree importance를 합산하여 평균 계산

## 3. Gradient Boosted Feature Importance

- GBT 알고리즘에서 각 Tree의 feature importance를 합산하여 전체 importance를 계산하는 방법
- 구체적인 방법
    - GBT훈련
    - 각 tree importance값 합산

## 4. L1/ L2 Regularization

- 정규화를 포함한 regression model을 사용하여 feature selection , L1정규화는 feature coeffecient를 0으로 만들수있금
- 방법
    - L1/L2 Regularization을 포함한 regression 모델 학습
    - Regularzation coeffecient가 각 feature에 적용

## 5. Recursive Feature Elimination with Cross -Validation

## RFE 방식

- 모델 훈련을 반복하면서 반복적으로 특성을 제거하는 동시에 교차검증을 사용하여 모델의 성능을 평가하는 방법
- 구체적인 방법
    1. 모든 feature 사용하여 모델 훈련
    2. Feature importance 순서대로 특성 제거
    3. Cross validation 하여 평가
    4. 최적 Feature 수 선정

# Feature Engineering # Feature Reduction

# Feature Reduction이란?

- 데이터의 feature 수를 줄이는 과정을 의미
- 데이터셋에서는 여러 특성이 포함되는데 모든 특성이 유용하거나 필요하지는않음
- **불필요한 특성을 제거거나 결합해서 새로운 특성을 생성하는 식**

## Feature Selection과의 차이는?

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2033.png)

# 2. PCA

- 선형 차원 축소방법
- 데이터 분산이최대가 되는 방향으로 데이터를 투영하여 원본데이터 차원 축소
- 정보손실을 최소화하면서 차원 축소
- 수식
    - 데이터의 X의 공분산 행렬계산
    - 공분산 행렬의 고유값과 고유벡터 계산
    - 고유값이 큰 순서대로 K개의 고유벡터 선택
    - 선택된 고유벡터를 데이터로 투영하여 차원 축소
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2034.png)
    

# 3. LDA : Linear Discriminat Analysis

- 분류문제에 적합
- 클래스간 분산은 최대로 , 클래스 내 분산을 최소로 만드는 방향으로 데이터를 투영하는 기법
- 각 클래스를 잘 구분할수 있는 특성을 생성하는 것이 목표
- 수식
    - 클래스 내에 분산행렬과 클래스간 분산 행렬 계산
    - 두행렬 고유값과 고유벡터 계산
    - 고유값이 큰 순서대로 K개의 고유벡터 선택
    - 선택된 고유벡터롤 데이터를 투영하여 차원 축소
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2035.png)
    

# 4. T-Distribute Stochastic Neighbor Embedding (T-SNE)

- 고차원 데이터의 구조를 저차원에서 유지하려는 비선형 차원 축소 기법
- 원본 고차원 공간에서의 데이터 포인트간의 유사도와 저차원 공간에서의 데이터 포인트간의 유사도를 비슷하게 하려는 것
- 수식
    1. 고차원에서 데이터포인터간 유사도 → 가우시안 분포사용하여 계산
    2. 저차원에서 데이터포인트간 유사도를 t-분포를 사용하여 계싼
    3. 1,2에서 계산한 식의 차이가 최소가 되도록 데이터포인트 조정
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2036.png)
    

# 5. AutoEncoder

- 딥러닝 기반의 비지도 학습모델
- 입력데이터를 압축해 디코딩하여 복원
- 압축된 데이터가 특성축소가능
- 장점: 비선형관계 포착가능하며 대용량 데이터에 효과적

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2037.png)

# Feature Engineering # Data Augmentation

# Data Augmentation?

- 기존 데이터셋을부터 추가적인 학습 데이터를 생성하는 기법
- 데이터에서 약간의 노이즈를 추가하거나 ,회적, 축소 ,확대 등의 원본데이터와 다른 새로운 데이터 생성
- 과정
    - 데이터 부족 문제 해결: 딥러닝 같은 복잡한 모델을 학습시킬때 충분한 양의 학습데이터가 필요
    - 오버피팅 방지 : 다양한 변형을 데이터로 학습을 시켜 일반적 특징 학습
    - 데이터 다양성증가 : 예측할수없는 다양성을 모의로 재현 가능
- 예시
    - 이미지 분류 → 회전, 확대 /축소 ,색상조정등
    - 자연어처리 → 문장순서 , 동의어 대체ㅔ
    - 음성인식 → 소음추가, 속도조절, 피치조절등

## Image Data Augmentation

- 회전
- 확대
- 반전
- 색상 변형
- cutout
- 효과
    - 다양한 각도, 크기 색상에서의 객체를인식하는 능력향상
    - 오버피팅 방지 및 데이터 다양성 증가
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2038.png)
    

## Text Data Augmentation

- back translation : 원본 텍스트를 다른언어로 번역후 다시 원래 언어로 번역
- 단어 삽입, 제거 , 교체 : 무작위로 단어 삽입하거나 제거한거나 동의어로 교체
- 문장 순서 변경 : 문장내에서 단어나 구의 순서 변경
- 효과
    - 다양한 문맥과 단어사용에 대한 이해도 향상
    - 데이터 증가

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2039.png)

## Audio Data Augmentaiton

- time stretching
    - 오디오 속도 조절하여 느리거나 빠르게 재생
- pitch shifiting
    - 오디오의 피치변경
- Background Noise Addition
    - 배경소음 추가
- 효과
    - 다양한 속도 ,피치 , 소음 환경에서의 오디오 인식능력 향상

## 정형 데이터 Augmenation

- Jittering
    - 데이터에 작은 노이즈를 추가하여 값을 약간 변경
- Resampling
    - 데이터에서 일부 행을 샘플링하여 데이터셋의 크기를 변경
- Feature Encoding
    - 카테고리 변수의 값을 변경하여 새로운 특성을 만듬
- 효과
    - 모델이 데이터의 작은 변화에 강인하게 만들고 데이터 분포 불균형 문제 해결하고 다양성 증가
    - 

# Feature Engineering # Data Scaling & Encoding

# Data Scaling 과 Encoding

- 데이터 scaling
    - 데이터의 feature값범위를 표준화하거나 정규화하는과정
        - 알고리즘의 성능과 학습속도에 큰 영향을 줄수 있ㅇ므
- Data Encoding
    - 카테고리형 변수를 숫자형 변수로 변환하는 과정
    - 숫자 데이터만 처리할수 있으므로 encoding필수과정

# 필요성

- 성능 최적화
    - 올바른 scaling과 encoding은 모델의 성능을 향상시킬수 있음
    - 잘못된 scalie또는 encoding은 알고리즘이 학습을 제대로 하지 못하게 만들수 있음
- 학습 속도 향상
    - scaling된 데이터는 경사 하강법과 같은 알고리즘의 수렴을 빠르게 만듬
- 데이터 이해력강화
    - 적절한 encoding을 통해 데이터와 구조의 관계를 더 잘 이해할수 있음

## 방법론

- Data Scaling
    - MINMAX
    - STANDARD
    - Robust
- Data Encoding

# 2. Data Scaling

## Min-Max Scaling

- 데이터를 0~1사이의 값으로 변환
- **단점 : 이상치에 민감**

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2040.png)

## Standard Scaling

- 데이터평균 0, 표준편차를 1로 변환
- 장점 : 데이터분포를 정규분포와 유사하게 만듦, 많은 머신러닝 알고리즘이 이런 분포를 가정하기 때문에 유용
- **단점 : 모든 feature의 범위가 동일하지 않을수 있음**

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2041.png)

## Robust Scaling

- 중앙값과 IQR를 사용하여 이상치 영향 최소화
- 장점: 이상치 영향 크게 안받음
- 단점:  데이터 분포를 왜곡할수 있음

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2042.png)

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2043.png)

# 3. Data Encoding

## Label Encoding

- 카테고리값 순차정수로 변환
- 장젖ㅁ : 간단함
- 단점 : 알고리즘이 숫자관의 관계 잘못해석 가능

## One-hot Encoding

- 카테고리값에 대해 binary feature 생성
- 장점 : 숫자간의 관계 고려 X
- 단점 : feature 수가 크게 증가할수 있음

## Ordinal Encoding

- 순서가 있는 카테고리 값 변환
- 장점 : 순서 정보를 유지하면서 카테고리형 변수를 숫자로 변환할수 있음
- 단점 : 순서가 없는 변수에 사용하면 오류 유발

# Model Training

## 정의

- 머신러닝에서 데이터를 사용하여 알고리즘을 훈련시켜 예측이나 분류등의 작업의 수행을 하는 모델을 생성하는 과정
- 목적 : 주어진 데이터를 기반으로 알고리즘이 일반적인 패턴을 학습하고 이를 바탕으로 미지의 데이터에 대해 예측하거나 분류하는 능력 개발
- 구성요소
    - 데이터 : 훈련데이터셋
    - 알고리즘 : 다양한 알고리즘, 특정 문제나 데이터유형에 따라 다름
    - 손실함수 : 모델의 예측값과 실제값사이의 차이를 측정하는 함수
    - 최적화 : 손실함수의 값을 최소화하는 방향으로 모델의 파라미터를 업데이터 하는 알고리즘

## 과정

- 데이터 준비
- 모델 선택
- 모델 초기화
    - 무작위 초기 파라미터값으로 초기화
- 훈련
    - 데이터를 모델에 전달
    - 손실함수 사용해 오차 계산
    - 최적화 사용하여 손실 최소화하는방향으로 모델 파라미터 업데이트
- 평가
- 튜닝
    - 하이퍼 파라미터, 혹은 feature engineerring

## 2. 문제 유형에 따른 모델 Traning

## Regression (회귀)

- 연속적인 값 예측
- 독립변수와 종속변수간의 선형관계 모델링
- 알고리즘
    - Linear Regression
    - Decision Treee Regression
    - Random Forest Regression
    - Support Vector Regression

## 분류

- 입력에 대해 두개이상의 클래스중하나를 예측
- Logistic 함수를 사용하여 이진또는 다중클래스 분류문제 해결
- 알고리즘
    - 로지스틱
    - Decision Tree
    - RandomForeset
    - K-Nearest Neighbors
    - Neural Network

## 클러스터링

- 비슷한 특성을 갖는 데이터를 그룹으로 묶는것
- 데이터를 k개의 클러스터로 분할하여 활용
- 알고리즘
    - K-Means : 데이터를 K개의 클러스터로 분류, 각 클러스터의 중심최적화
    - DBSCAN : 밀도 기반의 클러스터링 , Noise가 있는 데이터에 적합
    - Hierachical Clustering: 트리구조의 클러스터 생성하여 다양한 수준의 클러스터링 제공
- 예시
    - 고객분류 , 문서 군집화

## 생성형 (Generative)

- 새로운 데이터 생성 및 데이터 분포 학습
- 데이터 분포 학습하고 분포에 따라서 적절한 데이터를 생성하는 모델
- 알고리즘
    - Restricted Boltzmann Machine : 이진 값의 입력과 출력사이의 확률적 관계를 학습
    - VAE : 데이터의 Latent Space를 학습하여 새로운 데이터 생성
    - GAN : 생성자와 판별자 두 네트워크가 경쟁하면서 데이터 분포 학습

# 3. Training 유형

## Batch Training

- 데이터를 한번에 사용하여 모델 학습

## Oneline Training

- 데이터를 배치로 나누어 순차적으로 모델을 학습시키는 방법
- 미니 배치로 나누어서 진행 , 각 배치마다 가중치 업데이트
- 실시간 데이터 수신에서 효과적

## Transfer Learning

- 사전 훈련된 모델을 사용하여 새로운 문제에 대한 학습을 가속화하는 방법
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2044.png)
    

## Reinforcement Learning

- 에이전트가 환경과 상호작용하면서 보상 최대화하는 방법

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2045.png)

- 주로 Qlearning , DQN, Policy Gradient등이 있음

## Active Learning

- 모델이 스스로 학습에 사용할 데이터를 선택하는 방법
- 라벨링에 비용이 많이 드는 경우 스스로 필요한 샘플선택후 전문가에게 라벨링 요청하는 식으로 구성
- 불확실성이 높은 sample을 선호
- 예시
    - 의료 영상에서 분ㄴ석모델이 확신하지 못하는 경우 사용

## Ensemble Learning

- 여러개 모델 조합해 하나의 강력한 모델을 만드는 방법
- 여러개의 모델훈련후 그 결과 조합

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2046.png)

# Model Evaluation

- 모델의 성능 평가 및 검증
- Evaluation Metrics를 사용해 예측모델 평가
- 사용 이유
    - Overffitting방지
    - 모델선택
    - 성능향상 지표
    - 비즈니스 목표 달성
    - 신뢰성 및 투명성

# 2. 방법

## Holdout

- 데이터를 훈련과 valid로 한번만 분할
- 70~80% , 30~20% 정도로 사용
- 간단하고 빠르게 실행
- 단점 : 데이터 크기가 작은 경우 테스트 데이터의 결과가 불안정할수 있음

## Cross Validation

- 데이터를 여러부분으로 나누어 한 부분을 테스트 셋으로 사용하고 나머지 부분을 모델학습에 사용하고 반복
- K-fold
- Stratified k-fold
- Leave-one-out (LOO)
- Leave-P-Out (LPO)

## K-fold

- 데이터를 K부분으로 나누고 각 test하는 과정을 반복
- 장점: 모든 데이터가 훈련과 테스트에 모두 사용되므로 결과 안정적
- 단점: 모델을 K번학습해야하므로 계산 비용이 높음

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2047.png)

## Startified K-fold

- class 비율이 일정하게 유지되도록 데이터를 K개로 나누어 K-fold 수행하는 것
- 장점 : class imbalance데이터셋에 효과적
- 단점 : 계산 비용이 높음

## Leave-One-Out

- N개의 데이터중 하나의 데이터를 Test로 사용하고 나머지를 다 훈련에 사용
- 이과정을 N번 반복
- 장점 : 작은 데이터셋에 효과적
- 단점 : 매우 높은 계산 비용 발생
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2048.png)
    

## Leave-P-Out

- N개의 데이터포인트중 P개를 테스트로 사용하고 나머지 훈련에 사용
- 가능한 모든 조합에 대해 반복
- 장점 : LOO의 변형으로 P개의 데이터포인터에 대한 영향 조사가능
- 단점 : 계산비용 높음
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2049.png)
    

# 3. Evaluation Metrics

- Accuracy
- Precision
- Recall
- F1-score
- MSE
- MAE
- R-Square

## Confusion Matrix

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2050.png)

## Precision

- 모델이 양성으로 예측한것중 실제 양성의 비율
- P = TP/(TP+FP)

## Recall

- 실제 양성중 모델이 양성으로 예측한것의 비율
- R = TP / TP +FN

# F1-score

- precision과 recall의 조화평균
- f1 = 2*(P*R)/P+R

## ROC AUC curve

- threshold모델의 recall 대비 거짓양성비율을 나타내는 그래프
- 곡선아래의 영역 0.5~1사이값

## MSE (Mean Squared Error)

- 평균제곱오차
- 일반적인 회귀문제에서 널리 사용
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2051.png)
    

## MAE (Mean Absolute Error)

- 실제값과 예측값 차이를 절대적으로 평균한 값
- 큰 오차에 덜 민감한 평균지표가 필요할때 사용
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2052.png)
    

## R-Squared

- 모델이 데이터의 분산을 얼마나 잘 설명하는지 나타내는 지표
- 모델의 설명력 평가할때 사용
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2053.png)
    

# Model HyperParameter Tuning

## Hyper Parameter Tuning?

- 모델에 쓰이는 하이퍼 파라미터값 조정
- 모델훈련중에 자동으로 조정되지 않는 파라미터의미하며 학습전에 지정되어야함
- 예시
    - NN
        - Lr
        - BatchSize
        - layer
        - Number of Units per Layer 등등
    - DecisionTree
        - Maximum Depth
        - Minimum sample split
    - SVM
        - c , kernel 등
- 올바른 설정 ⇒ 모델의 성능에 큰 영향
- 학습 능력 제한하거나 overfitting 초래

# 2. 방법론

## Grid Search

- 하이퍼파라미터의 가능한 모든 조합 탐색하는 방식
- 사용자는 각 하이퍼파라미터 값을 명시적으로 제공하고 Grid Search가 조합의 모든 가능한 세트를 시도하여 최적의 성능을 얻음
- 원리
    - 모든 하이퍼 파라미터 조합에 대해 모델 학습
    - 각 학습마다 성능지표 사용하여 평가
    - 성능이 좋은 조합 선택
- 장점: 최적의 값을 놓지지 않음 ,재현 가능
- 단점: 탐색범위가 클경우 시간과 연산비용높음

## Random Search

- 하이퍼파라미터값 무작위로 선택하는 방식
- 각 하이퍼파라미터에 대한 분포를 지정하고 Random Search는 이 분포에서 값을 sampling하여 모델 평가
- 원리
    1. 각 하이퍼 파라미터 분포 지정
    2. 지정된 반복횟수, 혹은 시간동안 분포에서 무작위값 sampling
    3. sampling된 값으로 모델 학습시키고 성능 지표 사용하여 평가
    4. 가장 성능이 좋은 조합을 선택
- 장점 : Grid Search보다 비용 낮음, 다양한 조합 값 빠르게 탐색
- 단점 : 최적의 값이 탐색공간에 있더라도 Random으로 놓칠수 있음

## Bayesian Optimization

- 확률모델을 기반으로 하이퍼 파라미터의 최적조합을 탐색하는 방식
- 성능 불확실성을 모델링하고 이정보를 사용하여 당ㅁ에 시도할 하이퍼파라미터 조합 결정
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2054.png)
    
1. 가우시안 과정을 통해 확률모델로써 함수의 불확실성을 모델링
2. Acquistion function 을 사용하여 탐색할 위치를 결정
3. 선택된 위치에서 함수를 평가하고 확률모델 업데이트
4. 이 과정 반복
- 장점: 연산 비용이 높은 함수에 효과적 ,탐색과정에서의 불확실성 고려
- 단점 : 모델링 과정이 복합, 높은 차원의 탐색공간에서 덜 효과적

## Gradient-based Optimization

- 하이퍼 파라미터의 gradient정보를 사용하여 최적화 값을 탐색하는 방식
- 최근에는 자동미분같은 기술이 하이퍼파라미터 최적화에 사용되기 시작
1. 하이퍼 파라미터 초기값에서 시작
2. 해당값에서 gradient계산
3. gradient방향으로 하이퍼 파라미터를 업데이트
4. 최적의 값을 찾을때까지 이 과정을 반복
- 장점:  연속형 하이퍼파라미터에 효과적
- 단점 : gradient를 계산할수없는 경우나 불연속적은 하이퍼 파라미터에 적용하기 어려움

## Evolutionary Algorithms

- 자연선택원리 모방하여 하이퍼파라미터 최적조합 탐색
- 장점 : 여러해를 동시에 탐색가능
- 단점 : 연산비용높음

# Model Selection?

- 주어진 데이터에 대해 가장 잘맞는 머신러닝 알고리즘 선택하는 고자ㅓㅇ
- 데이터 특징을 잘표현할수있는 모델의 구조를 결정하고 최적의 하이퍼파라미터를 찾는것이 포함
- 일반화 오류를 최소화하고 , 새로운 데이터에 대한 모델의 예측성능 높이는것

# Tranin / Validation / Test Split

## Cross Validation

- 데이터셋 나누고 번갈아가면서 성능검증

## Informatiion Criterion Approaches

- AIC, BIC 등의 정보기준을 사용하여 모델의 복잡성과 성능상이의 균형을 찾음
- 위 기준으로 모델이 좋은 성능을 보상하고 ,과도하게 복잡한 모델에 대해서는 페널티 부여

## AIC

- 모델의 복잡성(파라미터수)모델의 적합도(모델의 데이터 설명력) 를 함께 고려
- Maximum likelihood에 대해 적합도를 제공하고 모델의 복잡성에 비례하는 패널티 부여
- 모델을 비교해서 AIC가 가장 낮은 모델 일반적으로 선택

## BIC

- AIC와 유사하나 샘플 크기에 대한 페널티 추가하여 더 큰 샘플 크기에 대해서는 더큰 페널티 부여
- AIC에 비해 간단한 모델 선호
- 모델을 비교해 BIC가 가장 낮은 모델 선택

## Grid/Random /Bayeisian

## Ensemble Method

# 주요 고려사항

- 데이터 크기, feature수, 예측 문제의 복잡성, 실행시간 ,해석가능성, 마지막으로 하드웨어 제약등 고려
- selection은 expermiment기반접근 + 통계적 방법론 혼합하여 수행

# MLOps에서 Model Selection

- 모델 성능 및 배포 가능성, 유지가능성, 확장성 및 비즈니스 요구 사항 충족 여부 포함한 다양한 측면
- 고려사항이 포함된 기술 ⇒ AutoML

# Ensemble & Auto ML

## Ensemble이란?

- 여러개의 머신러닝 모델을 결합하여 개별 모델보다 더 강력한 성능을 달성하는 기법
- 모델 학습과정에서 최적화
- 가정 : 여러개의 모델의 예측을 결합함으로써 각 모델이 가질수 있는 특정 오류 상쇄 가능
- 방법 : Bagging , Boosting ,Stacking

## Bagging?

- Bootstrap Aggregating의 줄임말
- 여러개의 모델이 서로 다른 샘플에 대해 학습하고 그 결과를 통합하는 방식
- Random Foreset 대표적 : 다수의 Decision Tree학습시키고 각 Tree의 예측을 평균내거나 다수결로 결정하여 최종 예측 도출
- 모델의 Varaiance를 감소시키고 overfitting방지하는데 유용

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2055.png)

## Boosting

- Weak Learners를 순차적으로 학습시키는 방법
- 이전 학습기가 잘못예측한 사례에 대해 더 많은 가중치를 두어 오류를 개선해 나감
- AdaBoost 대표적 : 잘못 분류한 데이터 포인트에 대해 더 많은 가중치를 할당해 새로운 분류기가 그 오류를 바로잡도록함
- XGBoost , Gradient Boosting : 각 반복에서 손실함수를 최소화하는 방향으로 모델을 업데이트
- 정확도가 올라갈수있느나 overfitting에 취약할수 있음

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2056.png)

## Stacking?

- 여러 다른 모델의 예측을 새로운 데이터로 사용하여 추가적인 모델을 학습시키는 방법
- 여러 모델의 예측을 입력으로 사용하여 최종 예측을 결정하는 모델
- 각기 다른 모델의 예측을 결합하여 더 정확한 예측을 생성하는데 사용

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2057.png)

# 2. AutoML?

- 머신러닝 모델을 개발하는 과정 자동화하기 위한 기술과 도구의 집합
- 다양한 단계를 진행하는 복잡성을 줄이고 사용자가 빠르게 효율적으로 모델을 구축할수 있게 도움
- ML전문가가 아니라도 머신러닝모델 개발 및 배포 할수있음

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2058.png)

## 핵심기능

- 데이터 전처리
- feature enginerring
- Model selection
- Model Learning & Hyperparameter Tunign
- Model Evaluation등

## 대표 도구

- google cloud AutoML
- H2O AUtoML
    - stacking 기법을 사용하여 최종모델 사용
- Auto-sklearn
    - scikit-learn기반의 AutoML도구로 특히 분류와 회귀문제에 적합

# 3. Ensembles Vs AutoML

### 차이점

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2059.png)

# 실습

# HR DATA 실습

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2060.png)

- 직원데이터 ⇒ 퇴사 여부 파악

## 1. 중복 데이터 확인

```python
print("중복된 항목 수 :", len(df[df.duplicated()]))

# 1. 머신러닝 데이터를 만드는, 데이터 준비 과정에서 문제가 없었는지.
# 2. 수집 시스템 자체에 문제가 없는지도.
```

- 중복데이터가 있을시 데이터 준비과정 문제?
- 수집데이터에 문제?

## 2. EDA

- df.info()
- 데이터 형태 ,type, null, 개수등확인
- 전체 데이터 확인

```python
# categorical and numeric columns
for column_name in list(df.columns):
    print(column_name, df[column_name].dtype, df[column_name].unique())
```

## 카테고리형과 숫자형(numeric)구분

```python
# categorical -> object
# numeric -> int64

list_casting_as_object = ["Education","EmployeeNumber","EnvironmentSatisfaction","JobInvolvement","JobLevel","JobSatisfaction","PerformanceRating","RelationshipSatisfaction","StockOptionLevel","WorkLifeBalance"]
for column_name in list_casting_as_object:
  df[column_name] = df[column_name].astype("object")
```

```python
list_categorical_columns = list(df.select_dtypes(include=['object']).columns)
list_numeric_columns = list(df.select_dtypes(include=['int64']).columns)
target_column = "Attrition"
print(len(df.columns))
print(len(list_categorical_columns))
print(len(list_numeric_columns))
```

# 종속데이터 확인 (target데이터)

```python
sns.countplot(x=target_column, data=df)
```

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2061.png)

- [ ]  불균형 데이터
- 추가적인 sampling 필요한가?
- yes 로 있는데이터가 이미 뚜렷한가?
    - 굳이 안해도될수도 있음
- 혹은 ML학습시 cost-sensitive (weight class)등 적용가능

# 독립데이터 확인

```python
# 분석전 target column을 categorical list에서 제외
list_categorical_columns.remove(target_column)

df[list_categorical_columns].nunique().sort_values()

Over18                         1
PerformanceRating              2
OverTime                       2
Gender                         2
BusinessTravel                 3
Department                     3
MaritalStatus                  3
RelationshipSatisfaction       4
JobSatisfaction                4
WorkLifeBalance                4
StockOptionLevel               4
JobInvolvement                 4
EnvironmentSatisfaction        4
Education                      5
JobLevel                       5
EducationField                 6
JobRole                        9
EmployeeNumber              1470
dtype: int64
```

- unique가 1개? 의미가 딱히없음 → 제거
- employ number? ⇒ ID이므로 굳이 필요가없음

```python
# 불필요한 컬럼을 제거
# Over18은 1개 값만 갖으므로 제거
# EmployeeNumber는 고유한 id 값으로 제거
df = df.drop(["Over18", "EmployeeNumber"], axis=1).copy()
list_categorical_columns.remove("Over18")
list_categorical_columns.remove("EmployeeNumber")
print(list_categorical_columns)
```

```python
len(list_categorical_columns)

>> 16
```

## 각 데이터 별로 count value 값 확인

```python
# cateogircla column 별 분포 확인
plt.figure(figsize=(20,20))
x = 1
plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.8, wspace=0.2)
for column_name in list_categorical_columns:
    plt.subplot(4,4,x)
    x = x+1
    df[column_name].value_counts().plot(kind='bar')
    plt.title(column_name)
plt.show()
```

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2062.png)

- 남자가 더 많다
- job role은 특정 직업이 많고 다양한 데이터의 value 분포확인 가능

# Target 과 categorical 관계 분포 확인

```python
# categorical column과 dependent data(target column) 분포 분석
plt.figure(figsize=(30,30))
x = 1
#plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.8, wspace=0.2)
for column_name in list_categorical_columns:
    x = x+1
    df.groupby([column_name, "Attrition"]).count()["Age"].unstack().plot(kind='bar', stacked=True)
    plt.title(column_name)
plt.show()
```

- 위 방법으로 시각화시 데이터의 의미가 전부 달라 이해하기 어려울수도 있음

# Crosstab으로 비율화

```python
pd.crosstab(df[target_column], df["WorkLifeBalance"], normalize="index")
```

```python
pd.crosstab(df["WorkLifeBalance"], df[target_column], normalize="columns")
```

```python
# categorical column과 dependent data(target column) 분포 분석
plt.figure(figsize=(30,30))
x = 1
#plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.8, wspace=0.2)
for column_name in list_categorical_columns:
    x = x+1
    #df.groupby([column_name, "Attrition"]).count()["Age"].unstack().plot(kind='bar', stacked=True)
    #pd.crosstab(df[target_column], df[column_name], normalize="index").plot.bar()
    pd.crosstab(df[column_name], df[target_column], normalize="index").plot.bar()
    plt.title(column_name)
plt.show()

#pd.crosstab(df[target_column], df[column_name], normalize="index").plot.bar()
```

## 통계적 방법 사용

```markdown
##### 카이제곱 검정 (chisqaure-test)
*   카이제곱 검정은 두 범주형 변수에 대한 분석 방법.
*   예를 들어, 성별에 따른 선호 영화 장르 비교 문제.
*   카이제곱 검정은 3가지 종류가 있으며, 데이터 수집 방법과 가설에 따라 나뉜다. 본 실습에서는 독립성 검정(test of independence)를 진행한다.
*   본 실습에서는 독립 변수중의 categorical column data와 Attrition 간의 관계를 검증.
*   pvalue는 0.05 이하인 경우, 귀무가설을 기각한다고 정한다. (pvalue 기준은 항상 다를 수 있음. 정확한 기준은 없으나 일반적으로 0.05 사용)
*   https://ko.wikipedia.org/wiki/카이제곱_검정
```

```markdown
##### 카이제곱 검정 추가 설명 : 3가지 유형
1.   적합도 검정 (Goodness of fit)
  - 변수 1개
  - 기존에 알려준 기준이 존재하는 검정
  - 예시) 상자 안에 공 **3개가 같은 비율로 알려져 있음**. 공 100개를 뽑았을 때, 각 색의 비율이 구해짐. -> 기존에 알려진 공 비율 분포를 따르는지 검정
  - 귀무가설 : 변수 X의 관측분포와 기대 분포가 동일
  - 대립가설 : 변수 X의 관측분포와 기대 분포가 동일하지 않음

2.   독립성 검정 (Test of independence)
  - 변수 2개
  - 범주형 **두 변수가 서로 연관되어 있는지 여부**를 검정
  - 예시) 성별과 흡연 여부 관계를 알고 싶어서 200명을 추출하여 조사한 경우.
  - 귀무가설 : 변수 X와 Y는 서로 독립
  - 대립가설 : 변수 X와 Y는 서로 독립이 아님
3.   동질성 검정 (Test of Homogeneity)
  - 변수 2개
  - 범주형 두 변수의 관계를 알기 위한 검정은 아님
  - 각 그룹들이 동질한지 알고 싶은 검정
  - 예시) 남자와 여자 흡연율 차이가 있는지 흡연율을 조사한 후, 두 그룹의 흡연율이 같은지 여부를 검정
  - 귀무가설 : 각 그룹의 확률분포가 동일
  - 대립가설 : 각 그룹의 확률분포가 동일하지 않음
```

```python
list_meaningful_column_by_chi = []
for column_name in list_categorical_columns:
  statistic, pvalue, _, _ = chi2_contingency(pd.crosstab(df[target_column], df[column_name]))
  if pvalue <= 0.05:
    list_meaningful_column_by_chi.append(column_name)
  print(column_name, ", ",statistic,", ", pvalue)
print("all categorical columns : ", len(list_categorical_columns))
print("selected columns by chi : ", len(list_meaningful_column_by_chi), list_meaningful_column_by_chi)
```

- chi2_contingency에는 crosstab으로 된 테이블형태가 필요함
- 이를통해서 카이제곱의 p value값을 얻을수있음
- p ≤ 0.05이하인 값만 뽑아서 사용하기로 하쟈

## 카테고리컬 데이터에서 종속변수와 연관성이 없어보이는 feature들을 줄일수 있었음

# Numerical Data EDA

## 1. 중복확인

```python
df[list_numeric_columns].nunique().sort_values()
```

- 1개거나 data 수에 거의 비슷하게 되어있다면 제거 혹은 확인

```python
# 불필요한 컬럼을 제거
# unique number가 1인 것들 제거
df = df.drop(["EmployeeCount", "StandardHours"], axis=1).copy()
list_numeric_columns.remove("EmployeeCount")
list_numeric_columns.remove("StandardHours")
print(list_numeric_columns)
```

## 2. 컬럼별 분포확인

```python
# numeric column 별 분포 확인
plt.figure(figsize=(20,20))
x = 1
plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.2, wspace=0.2)
for column_name in list_numeric_columns:
    plt.subplot(4,4,x)
    x = x+1
    df[column_name].value_counts().plot(kind='hist')
    plt.title(column_name)
plt.show()
```

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2063.png)

## Numeric 데이터는 데이터간의 상관관계가 있을수 있다.

- 확인corr

```python
# scatter plot 1
plt.figure(figsize=(7,7))
sns.scatterplot(data=df, x="MonthlyIncome", y="Age")
```

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2064.png)

- 만약 너무 높은 상관성을 보인다면 제거하는것도 필요
    - 모델이 편향이 될 가능성이 있음

```python
sns.pairplot(df[list_numeric_columns])
```

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2065.png)

- 위와 같이 한번에 확인할수도 있지만 컬럼수가 많을수록 파악하기 힘듬

## 따라서 correaltion 방식으로 확인하는것이 일반적

```python
df_corr = df[list_numeric_columns].corr()
plt.figure(figsize=(10,10))
sns.heatmap(df_corr, annot=True)
```

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2066.png)

# VIF 분석

- 일반적으로 10 이상인경우 다중공선성이 있다고 가정
- high correlation을 제거하기 전에 한번더 검정을 진행 (개인적인 판단)

```python
def caculate_vif(df_target):
  vif = pd.DataFrame()
  vif['VIF_Factor'] = [variance_inflation_factor(df_target.values, i) for i in range(df_target.shape[1])]
  vif['Feature'] = df_target.columns
  return vif

df_vif = df[list_numeric_columns].copy()
```

```python
caculate_vif(df_vif)

	VIF_Factor	Feature
0	25.496860	Age
1	4.721572	DailyRate
2	2.271398	DistanceFromHome
3	9.767720	HourlyRate
4	7.286833	MonthlyIncome
5	4.737393	MonthlyRate
6	2.675998	NumCompaniesWorked
7	13.578688	PercentSalaryHike
8	13.311109	TotalWorkingYears
9	5.302141	TrainingTimesLastYear
10	10.485271	YearsAtCompany
11	6.342271	YearsInCurrentRole
12	2.437206	YearsSinceLastPromotion
13	6.380952	YearsWithCurrManager
```

- Age가 10을 넘기기 때문에 Age를 지우고 다시한번 확인

```python
caculate_vif(df_vif.drop(["TotalWorkingYears","Age","PercentSalaryHike"],axis=1))

	VIF_Factor	Feature
0	4.365355	DailyRate
1	2.205211	DistanceFromHome
2	7.507071	HourlyRate
3	4.212697	MonthlyIncome
4	4.343285	MonthlyRate
5	2.263575	NumCompaniesWorked
6	4.710238	TrainingTimesLastYear
7	9.432830	YearsAtCompany
8	6.322306	YearsInCurrentRole
9	2.435655	YearsSinceLastPromotion
10	6.346938	YearsWithCurrManager
```

```python
list_numeric_feature_by_vif = list_numeric_columns.copy()
list_numeric_feature_by_vif.remove("TotalWorkingYears")
list_numeric_feature_by_vif.remove("Age")
list_numeric_feature_by_vif.remove("PercentSalaryHike")

list_numeric_feature_by_vif
```

- 위 10을 넘는 3가지를 제거해두고 나중에 성능을 더 높이거나 재구성할시 추가해도됌
- 하지만 지금은 제거를 하고 사용

# Target과 numeric 데이터 관계분포확인

```python
plt.figure(figsize=(20,20))
x = 1
plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.5, wspace=0.2)
for column_name in list_numeric_columns:
    plt.subplot(9,3,x)
    x = x + 1
    sns.violinplot(data=df,x=target_column,y=column_name)
plt.show()
```

![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2067.png)

# 3. 통계적 분석 사용

## ANOVA ( 일원분산분석)

- 카테고리별 numeric data분포 차이를 검증
- 하지만 전제가 필요함
    - 정규성, 등분산성, 독립성
- p value는 0.05이하인 경우 귀무가설 기각한다고 한다.
- [ ]  정규성 검정
    - 귀무가설 : 모집단의 분포는 정규분포이다
    - 검정 방법: shpiro-wilks test, qqplot등
    - pvalue 0.05이하면 귀무가설 기각 (기각 못해야 정규성 만족하는것임)
    
    ```python
    # shapiro wilk test 검정
    # 귀무가설 : 해당 데이터는 정규분포이다.
    # pvalue <0.05 이면 귀무가설 기각
    for column_name in list_numeric_columns:
      statistic, pvalue = stats.shapiro(df[column_name])
      if pvalue >= 0.05:
        print(column_name)
    print("end")
    ```
    
    - 없음
    - 그래서 데이터자체를 정규성을 조금 띄도록 바꾸는 방법이 존재
    - Box-Cox Transformation
    
    ```
    Box-Cox Transformation
    *   Box 와 Cox는 적절한 비선형 변환으로 정규성을 만족시킬 수 있다고 주장
    *   람다 값에 따라서 변수 변환이 결정되는 방식
    *   장점 : 반응 변수의 정규성을 만족시킬 수 있음
    *   단점 : 해석이 어려움
    *   https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation
    ```
    
    ```python
    # boxcox transformation
    for column_name in list_numeric_columns:
      condition = (df[column_name] > 0)
      statistic, pvalue = stats.shapiro(stats.boxcox(df.loc[condition, column_name])[0])
      if pvalue >= 0.05:
        print(column_name)
    print("end")
    ```
    
    ```python
    # log transfomration
    for column_name in list_numeric_columns:
      condition = (df[column_name] > 0)
      statistic, pvalue = stats.shapiro(np.log(df.loc[condition, column_name]))
      if pvalue >= 0.05:
        print(column_name)
    print("end")
    ```
    
    - 해도 정규성 만족 안함
    
    ## 정규성을 만족하지 못하므로 비모수 검정 방법 사용
    
    ```markdown
    ##### 크루스칼 왈리스 검정 Kruskall-Wallis test (비모수 검정)
    *   비모수 검정이기 때문에 정규성/등분산성을 만족하지 않아도됨
    *   단, 비모수 검정이기 때문에 검정 결과 신뢰도가 모수 검정보다 떨어짐
    ```
    
    ```python
    # kruskall 검정
    list_meaningful_column_by_kruskall = []
    
    for column_name in list_numeric_columns:
      list_by_value = []
      for value in df[target_column].dropna().unique():
          df_tmp = df[df[target_column] == value][column_name].dropna()
          list_by_value.append(np.array(df_tmp))
      statistic, pvalue = kruskal(*list_by_value)
      if pvalue <= 0.05:
        list_meaningful_column_by_kruskall.append(column_name)
      print(column_name, ", ",statistic,", ", pvalue)
    print("all categorical columns : ", len(list_numeric_columns))
    print("selected columns by kruskal : ", len(list_meaningful_column_by_kruskall), list_meaningful_column_by_kruskall)
    
    Age ,  43.06268844023747 ,  5.3013684961038114e-11
    DailyRate ,  4.767706640276287 ,  0.02899842966260463
    DistanceFromHome ,  9.225723965000004 ,  0.002386383151703113
    HourlyRate ,  0.06579158284658387 ,  0.7975657845068993
    MonthlyIncome ,  57.768241263784475 ,  2.948926498830519e-14
    MonthlyRate ,  0.3419741493810883 ,  0.5586919160154533
    NumCompaniesWorked ,  1.3669853624354948 ,  0.24233051615850376
    PercentSalaryHike ,  0.8190089297863566 ,  0.3654700908743005
    TotalWorkingYears ,  58.175049815828345 ,  2.3980118249072023e-14
    TrainingTimesLastYear ,  3.9351850554126635 ,  0.04728593768607503
    YearsAtCompany ,  53.26510705772158 ,  2.9143753542698205e-13
    YearsInCurrentRole ,  47.925653386167035 ,  4.426912787258302e-12
    YearsSinceLastPromotion ,  4.168984320865666 ,  0.04117047318275827
    YearsWithCurrManager ,  45.17087528790509 ,  1.8057061514626796e-11
    all categorical columns :  14
    selected columns by kruskal :  10 ['Age', 'DailyRate', 'DistanceFromHome', 'MonthlyIncome', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']
    ```
    
    ## 앞서 추출한 VIF 분석방식과 겹치는 것들 확인
    
    ```python
    # kruskall 검정과 vif 분석에서 Target Column과 유의미한 관계를 갖는 것들의 교집합
    list_meaningful_vif_and_kruskall = list(np.intersect1d(list_numeric_feature_by_vif,list_meaningful_column_by_kruskall))
    print(len(list_meaningful_vif_and_kruskall))
    
    ['DailyRate',
     'DistanceFromHome',
     'MonthlyIncome',
     'TrainingTimesLastYear',
     'YearsAtCompany',
     'YearsInCurrentRole',
     'YearsSinceLastPromotion',
     'YearsWithCurrManager']
    ```
    
    ## 얻은 insight
    
    ```markdown
    ##### Insight
    1.   Attrition(종속 변수)와 유의미한 관계를 갖지 않는 numerical data가 확인됨
    2.   feature selection 및 model optimization을 위해 1번에서 얻은 결과를 활용
    3.   numercial data 간의 다중공선성이 의심되는 변수들이 있음
    4.   kruskall 검정과 vif 분석에서 문제가 없는 numerial data는 총 8개
    ```
    
    # 4. Data Processing
    
    ## Missing Value 확인
    
    ```python
    df.isna().sum()
    ```
    
    ## Feature Transformation
    
    ```python
    Y = df[target_column]
    X = df.drop([target_column],axis=1)
    ```
    
    - Y가 이때 binary이기 때문에 label Encode나 One hot Encoder 사용해야함
    
    ```python
    le_encoder = LabelEncoder()
    Y_encoded = le_encoder.fit_transform(Y)
    Y_encoded
    ```
    
    ## Feature Selection
    
    ```python
    X_fs = df[list_meaningful_column_by_chi + list_meaningful_vif_and_kruskall]
    X_fs
    ```
    
    ## Onehot Encoding
    
    ```python
    X_base = pd.get_dummies(X)
    X_fs_final = pd.get_dummies(X_fs)
    ```
    
    ## Target에 대해 class imbalance가 드러나기 때문에 sampling
    
    - 하지만 전에 미리 data split
    
    ```python
    X_fs_final_train, X_fs_final_validation, y_fs_final_train, y_fs_final_validation = train_test_split(X_fs_final, Y_encoded, stratify=Y_encoded)
    ```
    
    ```python
    sme = SMOTEENN(random_state=random_state)
    X_fs_sme_sampling, y_sme_sampling = sme.fit_resample(X_fs_final_train, y_fs_final_train)
    ```
    
    ```python
    fig = plt.figure(figsize=(15,5))
    fig.add_subplot(121)
    sns.countplot(y_fs_final_train)
    plt.title("Before sampling for target label")
    
    fig.add_subplot(122)
    sns.countplot(y_sme_sampling)
    plt.title("After sampling for target label")
    ```
    
    ![Untitled](%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%A1%E1%86%BC%E1%84%91%E1%85%AE%E1%86%B7%E1%84%92%E1%85%AA%207e9a1d1e57d84041a65ce97659501a87/Untitled%2068.png)
    
    # 위 방법대로 진행했을시
    
    ```markdown
    ### 3-3 Summary
    ##### 총 3개 유형의 feature를 생성 -> model optimization 실험
    1.   No Feature Selection + No sampling (X_base, Y_encoded)
    2.   Feature Selection (X_fs_final, Y_encoded)
    3.   Feature Selection + Combined Sampling (X_fs_sme_sampling, y_sme_sampling / X_fs_final_validation, y_fs_final_validation)
    ```
    
    ```markdown
    ## 4. Model Analysis
    1.   base model (No Feature Selection + No sampling)
    2.   ML with Feature Selection (No samping)
    3.   ML with Feature selection and Combined Sampling
    4.   ML with Feature Selection and No Combined Sampilng + Cost Sensitive Learning
    ```
    
    # 5. 기본 모델 설정
    
    ```python
    list_model_metric = []
    cv = 3
    n_jobs = -1
    ```
    
    ```python
    X_train, X_validation, y_train, y_validation = train_test_split(X_base, Y_encoded, test_size = 0.3, random_state = random_state)
    
    xgb_clf = XGBClassifier(max_depth = 4, n_estimators=5)
    xgb_clf.fit(X_train, y_train)
    y_prd = xgb_clf.predict_proba(X_validation)[:,-1]
    score = roc_auc_score(y_validation, y_prd)
    print("roc_auc_score : ", score)
    list_model_metric.append(score)
    ```
    
    - roc_auc_score :  0.762148633038121
    
    ## 5.1 최적화 : ML feature Selection (NO sampling)
    
    ```python
    X_train, X_validation, y_train, y_validation = train_test_split(X_fs_final, Y_encoded, test_size = 0.3, random_state = random_state)
    ```
    
    ```python
    hyper_params_xgb = {"colsample_bytree":[0.8, 1.0],
                        "min_child_weight":[1.0,1.2],
                        'max_depth': [4, 6, 8, 10],
                        'n_estimators': [10, 15, 20, 25]}
    
    xgb_clf = XGBClassifier(max_depth = 4, n_estimators=5)
    grid_cv_opt1 = GridSearchCV(xgb_clf, param_grid = hyper_params_xgb, cv = cv, n_jobs = n_jobs)
    grid_cv_opt1.fit(X_train, y_train)
    print('best model hyper-parameter : ', grid_cv_opt1.best_params_)
    y_prd = grid_cv_opt1.predict_proba(X_validation)[:,-1]
    score = roc_auc_score(y_validation, y_prd)
    print("roc_auc_score : ", score)
    list_model_metric.append(score)
    ```
    
    - GrinSearchCV 사용
    - roc_auc_score : 0.805
    
    ## 5.2 최적화 : ML feature selection (No combined sampling + cost sensitive learning)
    
    ```python
    hyper_params_xgb = {"colsample_bytree":[0.8, 1.0],
                        "min_child_weight":[1.0,1.2],
                        'max_depth': [4, 6, 8, 10],
                        'n_estimators': [10, 15, 20, 25]}
    
    xgb_clf = XGBClassifier(max_depth = 4, n_estimators=5)
    grid_cv_opt2 = GridSearchCV(xgb_clf, param_grid = hyper_params_xgb, cv = cv, n_jobs = n_jobs)
    grid_cv_opt2.fit(X_fs_sme_sampling, y_sme_sampling)
    print('best model hyper-parameter : ', grid_cv_opt2.best_params_)
    y_prd = grid_cv_opt2.predict_proba(X_fs_final_validation)[:,-1]
    score = roc_auc_score(y_fs_final_validation, y_prd)
    print("roc_auc_score : ", score)
    list_model_metric.append(score)
    ```
    
    ## 5.3 최적화 : ML feature selection and Combined Sampling
    
    ```python
    hyper_params_xgb = {"colsample_bytree":[0.8, 1.0],
                        "min_child_weight":[1.0,1.2],
                        'max_depth': [4, 6, 8, 10],
                        'n_estimators': [10, 15, 20, 25]}
    
    xgb_clf = XGBClassifier(max_depth = 4, n_estimators=5)
    grid_cv_opt2 = GridSearchCV(xgb_clf, param_grid = hyper_params_xgb, cv = cv, n_jobs = n_jobs)
    grid_cv_opt2.fit(X_fs_sme_sampling, y_sme_sampling)
    print('best model hyper-parameter : ', grid_cv_opt2.best_params_)
    y_prd = grid_cv_opt2.predict_proba(X_fs_final_validation)[:,-1]
    score = roc_auc_score(y_fs_final_validation, y_prd)
    print("roc_auc_score : ", score)
    list_model_metric.append(score)
    
    >> 
    best model hyper-parameter :  {'colsample_bytree': 0.8, 'max_depth': 6, 'min_child_weight': 1.0, 'n_estimators': 15}
    roc_auc_score :  0.7450496407218474
    ```
    
    - 더 낮음
    
    ```python
    df_metric = pd.DataFrame({'Model':['basic', 'featue selection', 'feature selection and combinedsampling', 'feature selection and cost-sensitive learning'], 'roc_auc_score':list_model_metric})
    ax = df_metric.plot.barh(x='Model', y='roc_auc_score', rot=0, figsize=(10,5), legend=False)
    for bar in ax.patches:
        ax.annotate(format(bar.get_width(), '.4f'),
                       (bar.get_width(), bar.get_y() + bar.get_height() / 2),
                       size=15, xytext=(8, 0),
                       textcoords='offset points')
    
    # cost-sensitive learning이 의미가 없음
    # 이미 불균형중 minority 데이터 분포가 확실함
    ```
    

# Airplane Delay Regression ( READY)